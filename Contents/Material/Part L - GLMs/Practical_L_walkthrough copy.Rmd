---
title: "Practical L"
author: "Gerko Vink"
date: "Statistical Programming in R"
output: html_document
---

We use the following packages:
```{r}
library(mice)
library(magrittr)
library(caret)
library(ggplot2movies)
```

---

1. **Use the `brandsma` data from package `mice` to fit a logistic regression model for `sex` based on `lpo` (Language Post Outcome).**
```{r}
fit <- 
  brandsma %$%
  glm(sex ~ lpo, family=binomial(link='logit'))
```

---

2. **Inspect the model parameters. What do you conclude?**
```{r}
summary(fit)
```

With every unit increase in `lpo`, the logodds of gender increases by `r coef(fit)[2]`. 

---

3. **Obtain confidence intervals for the parameter estimates**
```{r}
confint(fit)
```

---

4. **Use the model parameters to predict the `sex` variable**
We can obtain predictions by using function `predict`. The default predictions are on the scale of the linear predictors; the alternative "response" is on the scale of the response variable. Thus for a default binomial model the default predictions are of log-odds (probabilities on logit scale) and type = "response" gives the predicted probabilities. 

To obtain the predicted logodds:
```{r}
pred.logodds <- 
  fit %>%
  predict()
head(pred.logodds)
```
and the predicted probabilities
```{r}
pred.prob <- 
  fit %>%
  predict(type = "response")
head(pred.prob)
```


```{r}
confusionMatrix(predict(fit))
```

tree rpart

---

The dataset `movies` contains data from imdb.com, a website that collects information about movies and allows users to rate these. It's in the `ggplot2movies` package, so if you do not have it, install that package first using `install.packages("ggplot2movies")`, then load the package. 

---

6. **The dataset is quite large, so it might be sluggish to work with. Create a new dataset that only has movies released after 2000 to speed up our other analyses. Also, we are interested in feature films, so throw away all movies shorter than 50 minutes and throw out all movies longer than 300 minutes. Use `range` to check that the new dataset is correct. We will work with this dataset from now on.**

```{r }
mv <- subset(movies, year > 2000 & length > 50 & length < 300)
range(mv$length)
range(mv$year)
```

---

7. **What movie has the most votes? **

```{r }
# which.max returns the index number of the highest number of votes. 
mv[which.max(mv$votes), "title"]

# By sorting on the number of votes we can obtain the top 10.
tail(mv[order(mv$votes), c("title", "votes")], n=10)
```

---

8. **Add the logarithm of the budget to the dataset. Change the `-Inf` values (use `is.infinite()` to `NA`.**

```{r }
mv$logbudget <- log(mv$budget)
mv$logbudget[is.infinite(mv$logbudget)] <- NA
```

---

9. **Does a having a bigger budget mean a higher or a lower rating on IMDB? Perform tests and make plots. You can also test and plot using the logarithm of the budget. **

```{r }
# Using the budget directly.
cor(mv$budget, mv$rating, use = "complete.obs")
cor.test(mv$budget, mv$rating, use = "complete.obs")
plot(mv$budget, mv$rating, col = rgb(.1, 0, .2, .3), pch=18)

# Using the logarithm.
cor(mv$logbudget, mv$rating, use = "complete.obs")
cor.test(mv$logbudget, mv$rating, use = "complete.obs")
plot(mv$logbudget, mv$rating, col = rgb(.1, 0, .2, .3), pch=18)
```

---

10. **Are drama's generally longer than animated films? Perform tests and make plots, for example `density` plots.**

```{r }
# Separate the two genres. Note: they are not mutually exclusive.
drama.mv <- mv[mv$Drama==1 & mv$Animation != 1, ]
anim.mv  <- mv[mv$Animation==1 & mv$Drama != 1, ]

# Test whether they differ.
t.test(drama.mv$length, anim.mv$length)
# Drama's are 102 minutes on average, animated movies are only 85 minutes. 

# lwd=3 makes the lines thicker, and main="Movie length" sets a title for the plot. 
plot(density(anim.mv$length), col="plum", lwd=3, main="Movie length")
lines(density(drama.mv$length), col="chartreuse", lwd=3)

# This function can create a legend. 
legend(120, .030, legend = c("Animated", "Drama"), fill = c("plum", "chartreuse"))
```

---

11. **Make a new factor `posneg` that divides the ratings up into `"positive"` and `"negative"`, with the cut-off at 6.5. Add it to the existing dataset.**

```{r }

# We can directly make the variable, or use cbind. 
mv$posneg <- factor(mv$rating > 6.5)
levels(mv$posneg) <- c("negative", "positive")

table(mv$posneg)
```

---

12. **Obtain a contingency `table` with `posneg` and the `mpaa` rating. Remove the entries without a rating and remove those with "NC-17" rating. Is the viewer response (positive/negative) to a movie dependent on the rating? **

---

```{r }
tab.full <- table(mv$posneg, mv$mpaa)
tab      <- tab.full[, 3:5]
tab

chisq.test(tab)
# There does not seem to be a relation. 
```

---

13. **Is there a relationship between `Comedy` and the `mpaa` rating? Does this make sense?**

```{r }
tab.full <- table(mv$Comedy, mv$mpaa)
tab      <- tab.full[, 3:5]
tab

chisq.test(tab)
# There does seem to be a relation. 
```

---

14. **Is there a relation between the length of the title of a movie and the length of the movie? (Hint: Use `nchar()`)**

```{r }
mv$titlelength <- nchar(mv$title)

plot(mv$titlelength, mv$length, col = rgb(.1, 0, .2, .3), pch=18)

cor(mv$titlelength, mv$length)
cor.test(mv$titlelength, mv$length)
```

---

End of Practical. 

---

3. **Load the `titanic` data set into your workspace with the following code:**
```{r}
con <- url("https://gerkovink.github.io/Statistical-Programming-with-R/Contents/Material/Part%20L%20-%20GLMs/Titanic.RData")
load(con)
```

4. **

```{r}
#contrasts(data$Sex)
#contrasts(data$Sex)

```


The following table shows numbers of occasions when inhibition (i.e., no flow of current across a membrane) occurred within 120 s, for different concentrations of the protein peptide-C. The outcome `yes` implies that inhibition has occurred.

    conc 0.1 0.5  1 10 20 30 50 70 80 100 150 
    no     7   1 10  9  2  9 13  1  1   4   3 
    yes    0   0  3  4  0  6  7  0  0   1   7

---

1. **Use logistic regression to model the probability of inhibition as a function of protein concentration.**

Let us inspect the data, proportions and `logodds`
```{r}
ex1 <- data.frame(conc = c(.1, .5, 1, 10, 20, 30, 50, 70, 80, 100, 150),
                  no = c(7, 1, 10, 9, 2, 9, 13, 1, 1, 4, 3),
                  yes = c(0, 0, 3, 4, 0, 6, 7, 0, 0, 1 ,7)) #create data
ex1$margin <- ex1$no + ex1$yes
ex1$p <- (ex1$yes / ex1$margin) #compute proportion
ex1$p
ex1$logit <- log(ex1$p / (1 - ex1$p)) #compute log odds
ex1$logit
```

There are a lot of zero proportions, hence the $-\infty$ in the logit. You can fix this (at least the interpretation of the `logodds`) by adding a constant (usually 0.5) to all cells conform the empirical `logodds` [(see e.g. Cox and Snell 1989)](http://www.amazon.com/Analysis-Edition-Monographs-Statistics-Probability/dp/0412306204). 

\[ \log\text{odds} = \log\left(\frac{r + 0.5}{n-r + 0.5}\right)\]

```{r}
ex1$logit.emp <- log((ex1$yes + .5) / (ex1$no + .5))
ex1$logit.emp #empirical log odds
```

Let us fit the model, we use logarithmic transformation for concentration to make the variable more normal and use total numbers (margin) as weights.
```{r}
fit <- glm(p ~ I(log(conc)), family=binomial, weights=margin, data = ex1)
summary(fit)
```

---

2. **In the data set `minor.head.injury` (from package `DAAG`), obtain a logistic regression model relating `clinically.important.brain.injury` to other variables. Patients whose risk is sufficiently high will be sent for CT (computed tomography). Using a risk threshold of 0.025 (2.5%), turn the result into a decision rule for use of CT.**

Let us fit the model, predict `clinically.important.brain.injury` by all other variables in the data.
```{r}
require(DAAG)
fit <- glm(clinically.important.brain.injury ~ ., family=binomial, data=head.injury) 
summary(fit)
```
A risk of 2.5% corresponds to the cutoff for a CT scan. This translates to a logit of $\log\left(\frac{.025}{1-.025}\right) = -3.663562$. In other words, any sum of variables that "lifts" the intercept above -3.66 would satisfy the cutoff. 

---


3. **Consider the moths data set from package `DAAG`.**

* **(a)** What happens to the standard error estimates when the `poisson` family is used in `glm()` instead of the `quasipoisson` family in the following analysis?

```{r}
fit <- glm(A ~ habitat + log(meters), family=quasipoisson, data=moths)
summary(fit)
```

The dispersion parameter is equal to the sum of squared of the residual divided by its degrees of freedom:
```{r}
disp <- sum(resid(fit, type="pearson")^2) / 32
disp 
```

The variance would decrease by a factor $\sqrt{`r disp`} = `r sqrt(disp)`$ if a poisson model were used. 

* **(b)** Analyze the $P$ moths, in the same way as the $A$ moths in (a) were analyzed. Comment on the effect of transect length. Set the reference category to `"Lowerside"`. 

First we need to relevel the habitats such that the reference category is Lowerside 
```{r}
moths$h.relevel <- relevel(moths$habitat, ref="Lowerside")
```

Next, we can model the $P$ moths
```{r}
fit <- glm(P ~ habitat + log(meters), family=quasipoisson, data=moths)
summary(fit)
```

For every meter increase in transect length, the number of moths increases with a factor $e^{`r coef(fit)[9]`} = `r exp(coef(fit)[9])`$.

---

4. **The factor `dead` in the data set `mifem` gives the mortality outcomes (live or dead), for 1295 female subjects who suffered a myocardial infarction. Determine ranges for `age` and `yronset`, and determine tables of counts for each separate factor. Decide how to handle cases for which the outome, for one or more factors, is not known. Fit a logistic regression model, beginning by comparing the model that includes all two-factor interactions with the model that has main effects only.**

The ranges for `age` and `yronset`
```{r}
require(DAAG)
sapply(mifem[, c("age", "yronset")], range)
```

The tables of counts for the seperate factors
```{r}
isfac <- unlist(lapply(mifem, is.factor)) 
lapply(mifem[, isfac==T], table) #exclude variable that are not factors
```

There are *many* `nk`'s. The easiest way to treat them in a logistic model is to consider them to be a bonafide answer category. You just seperate them from the observed cases. It is not the ideal situation, but under generous assumptions (that the missingness is completely randomly distributed), a conservative way to deal with missingness (less power).
```{r}
for(i in 4:10){
  mifem[,i] <- relevel(mifem[,i], ref="n")
  } #relevel all factors (columns 4 to 10) to have reference category no
fit1 <- glm(outcome ~ ., family=binomial, data=mifem) #fit main effects model
fit2 <- glm(outcome ~ (.)^2, family=binomial, data=mifem) #fit model with all 2-way interactions

compare <- anova(fit1, fit2) #compare their fit
compare
```

The difference can be considered significant  according to the deviance difference test ($\chi^2$ test with 1 df):
```{r} 
pchisq(compare$Df[2], compare$Deviance[2])
```

Crossvalidation indicates that model 2 is to be considered less reliable than model 1. The $\chi^2$ test is less reliable than crossvalidation and since model 2 encountered algorithmic problems (perfect prediction), I would trust model 1 over model 2. 
```{r warning=FALSE, cache=TRUE}
CVbinary(fit1)
CVbinary(fit2)
```

