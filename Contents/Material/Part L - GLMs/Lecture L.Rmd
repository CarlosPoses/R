---
title: "Generalized linear models"
author: "Gerko Vink"
date: "Statistical Programming with R"
output:
  ioslides_presentation:
    smaller: yes
    widescreen: no
    logo: logo.png
---

## Packages and functions used
```{r message=FALSE}
library(magrittr) # pipes
library(DAAG) # data sets and functions
```

- `glm()` Generalized linear models
- `predict()` Obtain predictions based on a model
- `confint()` Obtain confidence intervals for model parameters
- `coef()` Obtain a model's coefficients
- `DAAG::CVbinary()` Cross-validation for regression with a binary response


# Generalized linear models
## GLM's
We knew 
\[y=\alpha+\beta x+\epsilon\]

Now we consider 
\[\mathbb{E}[y] = \alpha + \beta x\]

They're the same. Different notation, different framework.

The upside is that we can now use a function for the expectation $\mathbb{E}$ to allow for transformations. This would enable us to change $\mathbb{E}[y]$ such that $f(\mathbb{E}[y])$ has a linear relation with $x$.

The function $f()$ we call the `link function`. This function tranforms the scale of the response/outcome to the scale of the linear predictor.

Examples: $f(x) = x, \\ f(x) = 1/x, \\ f(x) = \log(x), \\ f(x) = \log(x/(1-x)).$

## Link functions
![Overview of link functions, bluntly borrowed from Wikipedia](links.png)

## GLM's continued

Remember that
\[y=\alpha+\beta x+\epsilon,\]

and that 
\[\mathbb{E}[y] = \alpha + \beta x.\]

As a result
\[y = \mathbb{E}[y] + \epsilon.\]

and residuals do not need to be normal (heck, $y$ probably isn't, so why should $\epsilon$ be?)

## Binary dependent variables

Success vs Failure, e.g.

- pass or fail the exam

- smoker or non-smoker

- illness or not

$~$

Estimate probability of a "success" given set of predictors.

## Logistic regression model

Let $p=P(Y=success)$

$$p=\frac{e^{\alpha+\beta x}}{1+e^{\alpha+\beta x}}$$

or

$$\mbox{logit}(p)=\alpha+\beta x$$



## Logit

The logit is the log of the odds of a success

\[\text{logit}(p) = \log\left(\frac{p}{1-p}\right)\]

```{r include=FALSE,  dev.args = list(bg = 'transparent')}
p <- (1:999)/1000 
gitp <- log(p/(1 - p)) 
par(pty="s")
plot(gitp, p, xlab = "logit(p)", ylab = "p", type = "l", pch = 1)
```

But why so complicated?

## Logistic vs linear regression

To avoid negative probabiliy estimates!

```{r echo=F, out.width="100%", fig.align='center'}
knitr::include_graphics("Lin_vs_LogReg.png")
```


## Goodness-of-fit

`Deviance` is the logistic counterpart of the `Sum of Squares`. 

- Deviance is the (mis)fit of the observed values to the expected values. 
 
- The lower the deviance, the better the fit (Dev = 0 is perfect fit)


$~$

`AIC` is for model selection of nested models

- `AIC` value has no absolute interpretation 

- model with lowest `AIC` is the best

# Example
 
## Model

Predict probability not moving after anesthetic from concentration

```{r, message = FALSE}
anesthetic %>% dplyr::select(-move, -logconc) %>% head(n = 10)
```

## Interpretation in terms of the logit


With each unit increase in concentration: 

- the logit of `nomove` increases with 5.567

- the odds  of `nomove` increase with a factor `log(5.567) = 1.72`

```{r,  dev.args = list(bg = 'transparent')}
anesthetic %$% glm(nomove ~ conc, family = binomial(link="logit")) 
```


## Predicted probabilities

For `conc = 1`, the probability of `nomove` is 

$$p = \frac{e^{-6.469 + 5.567}}{1+e^{-6.469 + 5.567}}=0.29$$

```{r}
data.frame(conc = anesthetic$conc, pred = glm(nomove ~ conc, 
                                              family = binomial(link="logit"), 
                                              anesthetic)$fitted)[1:10, ] %>% 
  round(2)

```


## Significance of parameter estimates

```{r,  dev.args = list(bg = 'transparent')}
anesthetic %$% 
  glm(nomove ~ conc, family = binomial(link="logit")) %>% 
  summary() %>% 
  coef() %>% 
  round(3)
```

# Logistic multiple regression

## Explore your data


```{r}
head(frogs)
```


Make the relation as linear as possible and avoid skew

- after all you are assessing a (generalized) linear model
 
Use transformations to "make" things more linear / less skewed




## Checking the data
```{r,  dev.args = list(bg = 'transparent'), echo=FALSE}
with(frogs, pairs(cbind(distance, NoOfPools, NoOfSites, avrain, altitude, 
                        meanmax+meanmin, meanmax-meanmin), 
                  col = "gray", panel = panel.smooth, 
                  labels = c("Distance", "NoOfPools", 
                             "NoOfSites", "AvRainfall", "Altitude", 
                             "meanmax + meanmin", "meanmax - meanmin"))) 
```

## Resolving issues
```{r,  dev.args = list(bg = 'transparent'), echo=FALSE}
with(frogs, pairs(cbind(log(distance), log(NoOfPools), NoOfSites, avrain, altitude, 
                        meanmax+meanmin, meanmax-meanmin), 
                  col = "gray", panel = panel.smooth, 
                  labels = c("log(Distance)", "log(NoOfPools)", 
                             "NoOfSites", "AvRainfall", "Altitude", 
                             "meanmax + meanmin", "meanmax - meanmin"))) 
```

## All predictors
```{r echo=FALSE,  dev.args = list(bg = 'transparent')}
summary(frogs.glm0 <- glm(formula = pres.abs ~ log(distance) + log(NoOfPools) 
                          + NoOfSites + avrain +  I(meanmax + meanmin) 
                          + I(meanmax - meanmin), family = binomial, data = frogs)) 
```

## Subset of predictors
```{r echo=FALSE,  dev.args = list(bg = 'transparent')}
frogs.glm <- frogs %$% glm(pres.abs ~ log(distance) + log(NoOfPools) 
                           + I(meanmax + meanmin) + I(meanmax - meanmin),  
                           family = binomial)
frogs.glm %>% summary()
```

## Fitted values
```{r,  dev.args = list(bg = 'transparent')}
frogs.glm <- frogs %$% glm(pres.abs ~ log(distance) + log(NoOfPools) 
                           + I(meanmax + meanmin) + I(meanmax - meanmin),  
                           family = binomial)
frogs.glm %>% fitted() %>% head()
frogs.glm %>% predict(type = "response") %>% head()
frogs.glm %>% predict(type = "link") %>% head() # Scale of linear predictor 
```

## Fitted values with approximate SE's
```{r,  dev.args = list(bg = 'transparent')}
pred <- frogs.glm %>% 
  predict(type = "link", se.fit = TRUE)
data.frame(fit = pred$fit, se = pred$se.fit) %>% head()
```

## Confidence intervals for the $\beta$
```{r}
frogs.glm %>% confint()
frogs.glm %>% coef()
```

## Cross validating predictive power
```{r,  dev.args = list(bg = 'transparent')}
set.seed(123)
frogs.glm <- glm(pres.abs ~ log(distance) + log(NoOfPools), 
                 family = binomial, data = frogs)
cv <- CVbinary(frogs.glm)
```

The cross-validation measure is the proportion of predictions over the folds that are correct. 

## Cross validating predictive power
```{r,  dev.args = list(bg = 'transparent')}
frogs.glm2 <- glm(pres.abs ~ log(distance) + log(NoOfPools) 
                 + I(meanmax + meanmin) + I(meanmax - meanmin),
                 family = binomial, data = frogs)
cv <- CVbinary(frogs.glm2)
```

## Cross validating predictive power
```{r,  dev.args = list(bg = 'transparent')}
frogs.glm2 <- glm(pres.abs ~ log(distance) + log(NoOfPools) 
                 + I(meanmax + meanmin) + I(meanmax - meanmin),
                 family = binomial, data = frogs)
cv <- CVbinary(frogs.glm2, nfolds = 5)
```



  