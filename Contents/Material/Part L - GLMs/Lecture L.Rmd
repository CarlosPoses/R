---
title: "Generalized linear models"
author: "Gerko Vink"
date: "Statistical Programming in R"
output:
  ioslides_presentation:
    smaller: yes
    widescreen: no
    logo: logo.png
---

# Generalized linear models
## GLM's
We knew 
\[y=\alpha+\beta x+\epsilon\]

Now we consider 
\[\mathbb{E}[y] = \alpha + \beta x\]

They're the same. Different notation, different framework.

The upside is that we can now use a function for the expectation $\mathbb{E}$ to allow for transformations. This would enable us to change $\mathbb{E}[y]$ such that $f(\mathbb{E}[y])$ has a linear relation with $x$.

The function $f()$ we call the `link function`. This function tranforms the scale of the response/outcome to the scale of the linear predictor.

Examples: $f(x) = x, \\ f(x) = 1/x, \\ f(x) = \log(x), \\ f(x) = \log(x/(1-x)).$

## Link functions
![Overview of link functions, bluntly borrowed from Wikipedia](links.png)

## GLM's continued

Remember that
\[y=\alpha+\beta x+\epsilon,\]

and that 
\[\mathbb{E}[y] = \alpha + \beta x.\]

As a result
\[y = \mathbb{E}[y] + \epsilon.\]

and residuals do not need to be normal (heck, $y$ probably isn't, so why should $\epsilon$ be?)

## Logit
\[\text{logit}(p) = \log(\frac{p}{1-p}) = \log(p) - \log(1-p)\]
```{r echo=FALSE,  dev.args = list(bg = 'transparent')}
p <- (1:999)/1000 
gitp <- log(p/(1 - p)) 
par(pty="s")
plot(gitp, p, xlab = "logit(p)", ylab = "p", type = "l", pch = 1)
```

## Logit continued
Logit models work on the $\log(\text{odds})$ scale

\[\log(\text{odds}) = \log(\frac{p}{1-p}) = \log(p) - \log(1-p) = \text{logit}(p)\]

The logit of the probability is the log of the odds. 

Logistic regression allows us to model the $\log(\text{odds})$ as a function of other, linear predictors. 

## $\log(\text{odds})$ explained
```{r cache=TRUE,  dev.args = list(bg = 'transparent')}
logodds <- rep(NA, 1001)
for(i in 0:1000){
  p <- i / 1000 
  logodds[i + 1] <- log(p / (1 - p))
}
head(logodds)
tail(logodds)
```

## $\log(\text{odds})$ explained
```{r,  dev.args = list(bg = 'transparent')}
plot(0:1000, logodds, xlab = "x", ylab = "log(odds)", type = "l")
```

## logit$^{-1}$ explained
```{r cache=TRUE,  dev.args = list(bg = 'transparent')}
invlogit <- exp(logodds) / (1 + exp(logodds))
head(invlogit)
tail(invlogit)
```

## logit$^{-1}$ explained
```{r,  dev.args = list(bg = 'transparent')}
plot(0:1000, invlogit, xlab = "x", ylab = "log(odds)", type = "l")
```

# Logistic regression
## Logistic regression
With linear regression we had the `Sum of Squares (SS)`. Its logistic counterpart is the `Deviance (D)`. 

 -  Deviance is the fit of the observed values to the expected values. 
 
With logistic regression we aim to maximize the `likelihood`, which is equivalent to minimizing the deviance. 

The likelihood is the (joint) probability of the observed values, given the current model parameters.

In normally distributed data: $\text{SS}=\text{D}$.

## Logistic regression
```{r, message = FALSE}
require(DAAG) 
anesthetic
```

## Logistic regression
```{r,  dev.args = list(bg = 'transparent')}
fit <- glm(nomove ~ conc, family = binomial(link="logit"), data = anesthetic)
summary(fit)
```

## Logistic regression
```{r,  dev.args = list(bg = 'transparent')}
anestot <- aggregate(anesthetic[, c("move","nomove")],  
                     by = list(conc = anesthetic$conc), FUN = sum) 
anestot$conc <- as.numeric(as.character(anestot$conc)) 
anestot$total <- apply(anestot[, c("move","nomove")], 1 , sum) 
anestot$prop <- anestot$nomove / anestot$total 
anestot
```

## Logistic regression
```{r,  dev.args = list(bg = 'transparent')}
fit <- glm(prop ~ conc, family = binomial(link="logit"), weights = total, data = anestot)
summary(fit)
```

## Logistic multiple regression
Always try to make the relation as linear as possible 

- after all you are assessing a linear model. 

Do not forget that you use transformations to "make" things more linear

## Logistic multiple regression
```{r,  dev.args = list(bg = 'transparent'), echo=FALSE}
with(frogs, pairs(cbind(log(distance), log(NoOfPools), NoOfSites, avrain, altitude, 
                        meanmax+meanmin, meanmax-meanmin), 
                  col = "gray", panel = panel.smooth, 
                  labels = c("log(Distance)", "log(NoOfPools)", 
                             "NoOfSites", "AvRainfall", "Altitude", 
                             "meanmax + meanmin", "meanmax - meanmin"))) 
```

## Logistic multiple regression
```{r,  dev.args = list(bg = 'transparent'), echo=FALSE}
with(frogs, pairs(cbind(log(distance), log(NoOfPools), NoOfSites, avrain, altitude, 
                        meanmax+meanmin, meanmax-meanmin), 
                  col = "gray", panel = panel.smooth, 
                  labels = c("log(Distance)", "log(NoOfPools)", 
                             "NoOfSites", "AvRainfall", "Altitude", 
                             "meanmax + meanmin", "meanmax - meanmin"))) 
```

## Logistic multiple regression
```{r,  dev.args = list(bg = 'transparent'), echo=FALSE}
with(frogs, pairs(cbind(distance, NoOfPools, NoOfSites, avrain, altitude, 
                        meanmax+meanmin, meanmax-meanmin), 
                  col = "gray", panel = panel.smooth, 
                  labels = c("log(Distance)", "log(NoOfPools)", 
                             "NoOfSites", "AvRainfall", "Altitude", 
                             "meanmax + meanmin", "meanmax - meanmin"))) 
```

## Logistic multiple regression
```{r echo=FALSE,  dev.args = list(bg = 'transparent')}
summary(frogs.glm0 <- glm(formula = pres.abs ~ log(distance) + log(NoOfPools) 
                          + NoOfSites + avrain +  I(meanmax + meanmin) 
                          + I(meanmax - meanmin), family = binomial, data = frogs)) 
```

## Logistic multiple regression
```{r echo=FALSE,  dev.args = list(bg = 'transparent')}
frogs.glm <- glm(pres.abs ~ log(distance) + log(NoOfPools) + I(meanmax + meanmin) 
                 + I(meanmax - meanmin),  family = binomial, data = frogs) 
summary(frogs.glm) 
```

## Fitted values
```{r,  dev.args = list(bg = 'transparent')}
head(fitted(frogs.glm))
head(predict(frogs.glm, type = "response"))
head(predict(frogs.glm, type = "link"))  # Scale of linear predictor 
```

## Fitted values with approximate SE's
```{r,  dev.args = list(bg = 'transparent')}
pred <- predict(frogs.glm, type = "link", se.fit = TRUE)
head(data.frame(fit = pred$fit, se = pred$se.fit))
```

## Confidence intervals for the $\beta$
```{r}
confint(frogs.glm)
coef(frogs.glm)
```

## Cross validating predictive power
```{r,  dev.args = list(bg = 'transparent')}
frogs.glm <- glm(pres.abs ~ log(distance) + log(NoOfPools), 
                 family = binomial, data = frogs)
cv <- CVbinary(frogs.glm)
```

The cross-validation measure is the proportion of predictions over the folds that are correct. 



  